
# import os
# import tensorflow as tf
# import numpy as np
# import matplotlib.pyplot as plt
# import config, data_loader, model as model_builder
# import keras
# keras.config.enable_unsafe_deserialization()

# def decode_batch_predictions(pred, num_to_char):
#     input_len = np.ones(pred.shape[0]) * pred.shape[1]
#     # Greedy search: Ch·ªçn k√Ω t·ª± c√≥ x√°c su·∫•t cao nh·∫•t t·∫°i m·ªói b∆∞·ªõc
#     results = tf.keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :config.MAX_LABEL_LENGTH]
    
#     output_text = []
#     for res in results:
#         # Chuy·ªÉn s·ªë v·ªÅ l·∫°i k√Ω t·ª±
#         res = tf.strings.reduce_join(num_to_char(res)).numpy().decode("utf-8")
#         res = res.replace("[UNK]", "").strip() 
#         output_text.append(res)
#     return output_text

# def main():
#     # 1. Load l·∫°i t·ª´ ƒëi·ªÉn (ph·∫£i kh·ªõp v·ªõi l√∫c train)
#     print("üîπ ƒêang t·∫£i c·∫•u h√¨nh v√† t·ª´ ƒëi·ªÉn...")
#     train_labels = data_loader.clean_labels(config.TRAIN_LABELS)[1]
#     valid_labels = data_loader.clean_labels(config.VALID_LABELS)[1]
#     char_to_num, num_to_char = data_loader.build_vocabulary(train_labels + valid_labels)
    
#     # 2. Load Model
#     model_path = r"C:\Users\khact\Desktop\AI\src\checkpoints\best_model.keras"
    
#     if not os.path.exists(model_path):
#         print("Ch∆∞a t√¨m th·∫•y file model! H√£y ƒë·ª£i train.py ch·∫°y xong Epoch 1 ƒë√£ nh√©.")
#         return

#     # Load model v·ªõi custom layer CTCLayer
#     model = tf.keras.models.load_model(model_path, custom_objects={"CTCLayer": model_builder.CTCLayer})
#     print(f"ƒê√£ load model t·ª´: {model_path}")

#     # T√°ch ph·∫ßn d·ª± ƒëo√°n ra kh·ªèi ph·∫ßn t√≠nh Loss
#     prediction_model = tf.keras.models.Model(
#         inputs=model.inputs[0], 
#         outputs=model.get_layer(name="dense_1").output
#     )
#     # 3. L·∫•y ng·∫´u nhi√™n 5 ·∫£nh trong t·∫≠p Test ƒë·ªÉ th·ª≠
#     test_img_paths, test_labels = data_loader.clean_labels(config.TEST_LABELS)
#     indices = np.random.randint(0, len(test_img_paths), 5)
    
#     print("\n" + "="*40)
#     print("K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN")
#     print("="*40)

#     for i in indices:
#         img_path = test_img_paths[i]
#         true_label = test_labels[i]
        
#         # X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o
#         img = data_loader.preprocess_image(img_path)
#         img_batch = tf.expand_dims(img, axis=0) # T·∫°o batch size = 1

#         # D·ª± ƒëo√°n
#         preds = prediction_model.predict(img_batch, verbose=0)
#         pred_text = decode_batch_predictions(preds, num_to_char)[0]
        
#         print(f"·∫¢nh: {os.path.basename(img_path)}")
#         print(f"Nh√£n ƒë√∫ng:  {true_label}")
#         print(f"M√¥ h√¨nh ƒë·ªçc l√†:  {pred_text}")
        
#         # ƒê√°nh gi√° ƒë∆°n gi·∫£n
#         if true_label == pred_text:
#             print("CH√çNH X√ÅC TUY·ªÜT ƒê·ªêI")
#         else:
#             print("C√≥ sai s√≥t nh·ªè")
#         print("-" * 30)

# if __name__ == "__main__":
#     main()



def cer(reference, hypothesis):
    import numpy as np
    ref = list(reference)
    hyp = list(hypothesis)

    d = np.zeros((len(ref)+1, len(hyp)+1), dtype=np.uint32)
    for i in range(len(ref)+1):
        d[i][0] = i
    for j in range(len(hyp)+1):
        d[0][j] = j

    for i in range(1, len(ref)+1):
        for j in range(1, len(hyp)+1):
            cost = 0 if ref[i-1] == hyp[j-1] else 1
            d[i][j] = min(
                d[i-1][j] + 1,
                d[i][j-1] + 1,
                d[i-1][j-1] + cost
            )
    return d[len(ref)][len(hyp)] / max(1, len(ref))


def wer(reference, hypothesis):
    import numpy as np
    ref = reference.split()
    hyp = hypothesis.split()

    d = np.zeros((len(ref)+1, len(hyp)+1), dtype=np.uint32)
    for i in range(len(ref)+1):
        d[i][0] = i
    for j in range(len(hyp)+1):
        d[0][j] = j

    for i in range(1, len(ref)+1):
        for j in range(1, len(hyp)+1):
            cost = 0 if ref[i-1] == hyp[j-1] else 1
            d[i][j] = min(
                d[i-1][j] + 1,
                d[i][j-1] + 1,
                d[i-1][j-1] + cost
            )
    return d[len(ref)][len(hyp)] / max(1, len(ref))



import os
import tensorflow as tf
import numpy as np
import config, data_loader, model as model_builder
import keras
keras.config.enable_unsafe_deserialization()


def decode_batch_predictions(pred, num_to_char):
    input_len = np.ones(pred.shape[0]) * pred.shape[1]
    
    results = tf.keras.backend.ctc_decode(
        pred, 
        input_length=input_len, 
        greedy=True
    )[0][0][:, :config.MAX_LABEL_LENGTH]

    texts = []
    for r in results:
        text = tf.strings.reduce_join(num_to_char(r)).numpy().decode("utf-8")
        text = text.replace("[UNK]", "").strip()
        texts.append(text)
    return texts


def main():
    print("üîπ ƒêang load t·ª´ ƒëi·ªÉn...")
    train_labels = data_loader.clean_labels(config.TRAIN_LABELS)[1]
    valid_labels = data_loader.clean_labels(config.VALID_LABELS)[1]
    char_to_num, num_to_char = data_loader.build_vocabulary(train_labels + valid_labels)

    model_path = r"/home/tudong/src/checkpoints/best_model.keras"

    if not os.path.exists(model_path):
        print(" Model kh√¥ng t·ªìn t·∫°i!")
        return

    print(" Load model...")
    model = tf.keras.models.load_model(
        model_path,
        custom_objects={"CTCLayer": model_builder.CTCLayer}
    )

    prediction_layer = model.get_layer("predictions")
    prediction_model = tf.keras.models.Model(
        inputs=model.inputs[0],
        outputs=prediction_layer.output
    )

    test_img_paths, test_labels = data_loader.clean_labels(config.TEST_LABELS)
    indices = np.random.randint(0, len(test_img_paths), 5)

    print("\n========== K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN ==========")

    for i in indices:
        img_path = test_img_paths[i]
        true = test_labels[i]

        img = data_loader.preprocess_image(img_path)
        img_batch = tf.expand_dims(img, axis=0)

        preds = prediction_model.predict(img_batch, verbose=0)
        pred_text = decode_batch_predictions(preds, num_to_char)[0]

        print(f"\n·∫¢nh: {os.path.basename(img_path)}")
        print(f"Nh√£n ƒë√∫ng: {true}")
        print(f"Model ƒë·ªçc: {pred_text}")

        # print("‚úî Ch√≠nh x√°c" if pred_text == true else "‚ö† Sai")
        print("----------------------------------")

            # ===============================
    #   ƒê√ÅNH GI√Å TR√äN TO√ÄN B·ªò T·∫¨P TESTcc
    # ===============================
    print("\n===== ƒêANG ƒê√ÅNH GI√Å TR√äN TO√ÄN T·∫¨P TEST =====")

    total_cer = 0
    total_wer = 0
    n = len(test_img_paths)

    for i in range(n):
        img_path = test_img_paths[i]
        true = test_labels[i]

        img = data_loader.preprocess_image(img_path)
        img_batch = tf.expand_dims(img, axis=0)

        preds = prediction_model.predict(img_batch, verbose=0)
        pred_text = decode_batch_predictions(preds, num_to_char)[0]

        # C·ªông d·ªìn CER & WER
        total_cer += cer(true, pred_text)
        total_wer += wer(true, pred_text)

        # C√≥ th·ªÉ in ti·∫øn tr√¨nh n·∫øu dataset l·ªõn:
        if i % 200 == 0:
            print(f"ƒê√£ x·ª≠ l√Ω {i}/{n} ·∫£nh...")

    avg_cer = total_cer / n
    avg_wer = total_wer / n

    print("\n===== K·∫æT QU·∫¢ CU·ªêI C√ôNG =====")
    print(f" CER trung b√¨nh: {avg_cer:.4f}")
    print(f"WER trung b√¨nh: {avg_wer:.4f}")
    print("=====================================")



if __name__ == "__main__":
    main()
